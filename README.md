âœ… Metadata-Driven Job Orchestration

Transformation jobs are defined in a centralized Job Dictionary

Airflow dynamically creates Databricks jobs based on job metadata

Easily add/remove jobs without modifying DAG logic

âœ… Custom Airflow Operators

YouTube API Operator â€“ Extracts video search results & category metadata

Databricks Job Create Operator â€“ Creates Databricks jobs dynamically

Databricks Job Run Operator â€“ Executes jobs using retrieved job IDs

Databricks Get Job IDs Operator â€“ Fetches job IDs via XComs

âœ… Stateful & Idempotent Execution

Job status is updated after successful execution

Prevents duplicate processing across DAG runs

âœ… Clean Task Dependency Management

Uses EmptyOperator for DAG boundaries

Controlled task chaining ensures reliable execution order

--------------------------------------------------------------
ğŸ”„ DAG Workflow

Start DAG

Execute a sample Python task

Extract YouTube video search data â†’ store in S3

Fetch YouTube video categories â†’ store in S3

Dynamically create Databricks transformation jobs

Update job status after successful creation

Retrieve Databricks job IDs

Execute Databricks jobs

End DAG

ğŸ“ˆ Use Cases

Automated ingestion of external API data

Metadata-driven transformation pipelines

Orchestrating Databricks workloads using Airflow

Building scalable, production-ready ETL systems

ğŸ” Configuration Notes

YouTube API credentials configured via Airflow connections

AWS credentials handled through Airflow connections / IAM roles

Databricks authentication managed via Airflow connections
